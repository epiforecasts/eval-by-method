# Review Response Analysis

## Summary

This document analyzes 40 specific suggestions from reviewers and the editor for the manuscript "The influence of model structure and geographic specificity on predictive accuracy among European COVID-19 forecasts."

**Distribution of comments:**
- Reviewer 1: 26 suggestions
- Reviewer 2: 5 suggestions  
- Reviewer 3: 6 suggestions
- Editor: 3 technical requirements

## Detailed Analysis Table

| Reviewer | Suggestion | Impact | Strategy | Next Step |
|----------|-----------|---------|----------|-----------|
| 1 | make better use of the result figures and tables, and incorporate subheadings to better guide readers to different aspects of the results | Medium - Affects readability and structure of results section, but doesn't change scientific content. Better organization would improve clarity for readers. | Accept - Restructure results section with clear subheadings (e.g., 'Model characteristics', 'Descriptive performance trends', 'Adjusted model estimates'). Explicitly reference figures/tables when introducing each result subsection. | Draft outline of results section with proposed subheadings and map existing paragraphs to new structure |
| 1 | including the regression formula for GAMM in the main text | Medium - Currently formula only in code. Including it in main text will improve transparency and reproducibility, helping readers understand exactly what was modeled. | Accept - Add formal mathematical notation of the GAMM formula in Methods section, showing the full model with all terms (fixed effects, random effects, smoothing terms). | Write out the mathematical formula from model-wis.R code (line with m.formula) in LaTeX notation for insertion into Methods |
| 1 | do another round on the language used for fixed/random effects/explanatory variables/covariates and standardise the terms throughout the text | Medium - Inconsistent terminology causes confusion. The manuscript uses 'explanatory variables', 'covariates', 'factors', and 'random effects' somewhat interchangeably. | Accept - Conduct systematic review of terminology. Use: 'explanatory variables' for overall, 'random effects' for grouping factors in GAMM, 'smooth terms' for splines. Create glossary for revision tracking. | Search manuscript for all instances of these terms and create mapping document showing current usage and proposed standardized term |
| 1 | random effect for location+time is effectively 'it's always hard/easy at this place at this time', but provides no further insight to what makes it hard or easy. Maybe instead of a spline of time with fixed knots = 40, you could discretely divide time into phases dominated by different circulating strains | High - This is a fundamental critique of model specification. Current approach: s(time, by=location, k=40). Changing to discrete epidemic phases would require major reanalysis and might change results. | Partly accept - Acknowledge the limitation in Discussion but retain current approach. Justify that country-specific time trends account for unmeasured time-varying factors. Note that variant phases as future work, but discrete phases require subjective choices about timing/thresholds. | Add paragraph to Discussion explaining rationale for continuous time smooths vs discrete phases, citing data-driven nature of current approach |
| 1 | I'm in general curious why using an additive model with smoothing splines. Intuitively I would expect most of the modelled effects to have fairly simple relationships with predictability | Medium-High - Questions the fundamental modeling approach. GAMMs were chosen to handle non-linear relationships (e.g., horizon effects) and complex nested structures, but reviewer suggests simpler approach might suffice. | Accept - Add justification in Methods explaining: (1) horizon effects likely non-linear, (2) time trends within countries are complex, (3) GAMMs allow data to determine functional form rather than imposing parametric assumptions, (4) model diagnostics support approach. | Write new paragraph in Methods section justifying GAMM choice, potentially with reference to preliminary analyses or theoretical expectations |
| 1 | Shouldn't the effect of model and methods be hierarchical, since each model is assigned to a class of methods? | High - This is a structural question about the GAMM. Current model: s(Model, by=Method, k=40) partially addresses this by allowing model effects to vary by method. But reviewer suggests fully hierarchical random effects structure. | Partly accept - The current specification does create method-specific model effects (models nested within methods). Add clarifying text explaining this is effectively hierarchical. Consider sensitivity analysis with fully nested random effects if computationally feasible. | Review current model specification in model-wis.R and write clear explanation of how current approach handles hierarchical structure |
| 1 | a better narrative is to say you must adjust for confounding factors to be able to compare forecast performance of your sampled models, thus the GAMM approach is necessary | Medium - This is about framing. Strengthens rationale for the analytical approach by emphasizing necessity rather than preference. | Accept - Revise Introduction/Methods to emphasize that comparisons across diverse targets/times REQUIRE adjustment. Highlight that unadjusted comparisons are misleading due to confounding. Frame GAMM as essential rather than optional enhancement. | Revise transition from Introduction to Methods, and opening of Results, to emphasize necessity of adjustment for valid comparisons |
| 1 | Are there any models with sub-national forecast targets? Related, did you find any effect of country population size on WIS among single-country models? | Low-Medium - These are additional analyses not currently in manuscript. Sub-national: none in European Hub. Population size: not analyzed but could be explored as supplementary. | Accept - Add statement in Methods/Results that no sub-national targets exist in dataset. For population size: conduct supplementary analysis or acknowledge as limitation if no signal found. | Check data for any sub-national forecasts. Run quick analysis of population size effect on single-country model performance. |
| 1 | I think you need to more thoroughly justify why only evaluate with WIS as opposed to or in addition to eg CRPS | Medium - WIS is standard for interval forecasts but reviewer questions why not CRPS (continuous ranked probability score). WIS is specifically designed for quantile forecasts; CRPS requires predictive distributions. | Accept - Add justification in Methods: WIS is proper scoring rule for interval forecasts, matches the quantile forecast format teams submitted, allows log transformation. Note that CRPS would require distribution assumptions/approximations. | Write paragraph justifying WIS choice, with citations to Bracher et al. 2021 and discussion of alignment with forecast format |
| 1 | Presumably all forecast intervals are for negatively binomial distributed count data (cases or death)? Please explicitly state this | Low-Medium - This is about data type specification. Forecasts are of counts, but teams don't necessarily specify their assumed distribution (quantiles are distribution-free). | Partly accept - Clarify that forecasts are of count data (incident cases/deaths) but note that quantile forecasts don't require teams to specify distributional assumptions. Observations are counts; forecasts are quantile predictions. | Add sentence in Methods data section explicitly stating forecasts and observations are of integer count data |
| 1 | Why log link in the GAMM? Since WIS is already calculated on log scale, it's not intuitive to me why the modelled effects are expected to be multiplicative | Medium-High - This questions the link function choice. Log-link with log-scale WIS means modeling multiplicative effects on an already log-transformed score. Could consider identity link instead. | Accept - Add justification: log-link accounts for right-skew in WIS distribution, ensures positive predictions, and multiplicative effects are interpretable as proportional changes in score. Alternatively, could re-run with identity link as sensitivity check. | Check WIS distribution diagnostics (histogram, QQ plots). Write justification for log-link or consider re-running with identity link if computational time permits |
| 1 | In the QQ plot in your supp results it looks like dispersion of WIS score in the fitted GAMM is a bit off? | Medium-High - Model diagnostics concern. If model fit is poor, this undermines inferences. Need to check supplementary figures and assess whether this is a serious problem. | Accept - Examine QQ plots carefully. If concerning: (1) try alternative link function or error distribution, (2) acknowledge limitation explicitly in Methods/Discussion, (3) note that point estimates may be robust even if uncertainty estimates are affected. | Review Supplementary Figures 4 and 5 (model diagnostics). Assess severity of QQ plot deviation and whether alternative specifications improve fit |
| 1 | it is vague that by model structure you mean mechanistic vs statistical etc, and by forecast target you mean single vs multi country. I think you need to word this sentence to more accurately reflect the language used later on | Low - Lines 16-17 in abstract. Simple wording fix for clarity. | Accept - Revise abstract to specify 'model structure (agent-based, mechanistic, semi-mechanistic, statistical)' and 'geographic specificity (single versus multiple countries)'. | Revise lines 16-17 of abstract with explicit terminology |
| 1 | I think it's worth expanding this paragraph to also point out that different models are by design meant to be good at different things | Low-Medium - Background section enhancement. Adds nuance about model purposes beyond forecasting (scenarios, counterfactuals, etc.). | Accept - Add 2-3 sentences to paragraph around line 69 noting that models serve multiple purposes, forecasting is one use case, and structural differences may be optimized for different objectives. | Draft additional sentences for Background section with relevant citations (e.g., Becker et al. 2021 already cited, or others on model purposes) |
| 1 | I'm not fully convinced that target-specificity is necessarily linked to how many target countries the model predicts to | Medium-High - This challenges the operationalization of a key concept. The reviewer argues single-country doesn't necessarily mean more context-specific modeling. | Partly accept - Acknowledge this limitation in Discussion. Clarify that we use number of targets as a PROXY for specificity, but recognize it's imperfect. Note that we lack data on actual model customization practices. Soften claims about 'specificity' vs 'number of targets'. | Add discussion paragraph acknowledging limitation of using target count as proxy for specificity; note ideal data would include detailed model customization metadata |
| 1 | if you analyse forecast performance only among models submitted to the same round of forecast for the same target, you do not need to model confounding factors... I think a stronger narrative is that post-hoc modelling of forecast performance enables you to compare across targets and time periods | Medium - Strengthens the framing of why the study approach is valuable. Current manuscript emphasizes comparability but could better emphasize the enabling of broader comparisons. | Accept - Revise lines 98-101 to emphasize that GAMM approach enables comparisons across all targets/times/models simultaneously, which wouldn't be possible with stratified analyses. This is a strength not just a necessity. | Rewrite Introduction section around lines 98-101 to emphasize comparative advantage of model-based approach |
| 1 | are all 'others' model expert judgement based? Why not just call them expert-judged? | Low - Terminology clarification for one category with only 3 models. | Accept - Change 'Other' to 'Expert judgment' or 'Human judgment' throughout manuscript and in Supplementary Table 1. | Find-replace 'Other' category name to 'Human judgment' (or check if all 3 are indeed expert judgment) |
| 1 | give package and R version numbers | Low - Reproducibility enhancement. Should report software versions. | Accept - Add R version and key package versions (mgcv, dplyr, ggplot2, etc.) to Methods section. Can extract from sessionInfo() if available in output files. | Check if sessionInfo() was saved in output. If not, document current versions. Add to Methods computational environment description |
| 1 | I find forecastsMean column unnecessary, it doesn't show anything useful (and the sum doesn't make sense). You also need to explicitly explain what the numbers in brackets are | Low - Table 1 presentation. Removing unnecessary column and clarifying notation improves table. | Accept - Remove 'Mean forecasts' column from Table 1. Add footnote or caption text explaining that numbers in brackets are standard deviations for Mean WIS column. | Edit descriptive.R to modify Table 1 generation code; update table caption |
| 1 | If someone has just seen the figure without thoroughly reading the text, they might arrive at the conclusion agent-based models are generally better, or than single country models are better, which are not the case. I wonder if these descriptive visualisations of WIS are necessary | High - Figure 1 could mislead readers about main findings. This is a serious concern as figures are often viewed independently of text. | Partly accept - Keep Figure 1 (shows data characteristics) but add prominent caption note: 'Unadjusted means shown; adjusted estimates in Figure 2 show minimal differences between model types.' Could also consider moving to supplement or adding visual annotation about confounding. | Revise Figure 1 caption to include warning about unadjusted estimates and emphasize need to view alongside Figure 2 |
| 1 | I also suggest making the colour scheme more distinct between panels A and B. I also think panel C is unnecessary | Low-Medium - Figure 1 visual design. Panel C shows epidemic activity over time; reviewer thinks it's not informative. | Partly accept - Change color schemes so A and B are clearly distinct. Consider moving panel C to supplement rather than removing, as it provides epidemiological context. | Modify plotting code in descriptive.R to use different color palettes for panels A vs B |
| 1 | I think you need to highlight the observation about ensembling more | Medium - Discussion emphasis. Ensemble finding is important but somewhat buried. | Accept - Add or enhance dedicated paragraph on ensembling implications, potentially moving earlier in Discussion. Reference prior Hub work more prominently. | Revise Discussion structure to elevate ensemble discussion; consider adding to end of first Discussion paragraph as key takeaway |
| 1 | or maybe single-country models perform better because multi-country models are fitted to data from different ongoing epidemics, and their shared or hierarchical parameters may simply be not as well calibrated for a specific country | Medium - Alternative explanation for single-country advantage. This is a good mechanistic hypothesis to include. | Accept - Add this alternative explanation to Discussion paragraph around line 295. Note that we can't distinguish between bespoke features vs. training data advantages with current data. | Add 2-3 sentences presenting this alternative mechanism in Discussion |
| 1 | there is value in evaluating forecasts on the go as soon as you have data, so that you can retrospectively evaluate again in the future and understand the impact of reporting errors and corrections in the data | Low - Additional discussion point about real-time evaluation. | Accept - Add sentence to Discussion around line 303 noting value of evaluating with real-time data as well as retrospective data to understand data revision impacts. | Add sentence to Discussion paragraph on data revisions |
| 1 | I actually think the focus on just WIS over 4-weeks is a strength of this work | Low - Framing of scope. Reviewer sees focused scope as positive, not negative. | Accept - Revise Discussion to frame focused scope (4-week, WIS) as strength rather than limitation. Emphasize that clear research question drove these choices. | Revise Discussion section around lines 323-336 to reframe as strength |
| 1 | you can look into this if you fitted discrete time effect of different epidemic phases dominated by different variants | Low - Future work suggestion, relates to earlier point about time specification. | Accept - Add to future work/limitations: variant-specific phases could be explored in future analyses with appropriate phase definitions. | Add sentence to limitations/future work about variant-phase analysis |
| 2 | Model could be included within the main manuscript (As Plos Computational Biology format allows this), that would help the reader to follow the manuscript. A detailed description is also required | Medium - Same as Reviewer 1's request for GAMM formula. Improves clarity and transparency. | Accept - Add formal GAMM formula to main Methods section with detailed description of each term. Explain random effects structure, smoothing terms, and how they address research questions. | Write mathematical formula with accompanying paragraph explaining each component (same action as for Reviewer 1 suggestion) |
| 2 | How was the 'Model adjustment' performed? Make a detailed discussion about adjusted and non-adjusted scenarios with the corresponding model | Medium - Methods clarity. Need to explain more clearly what 'adjustment' means in the GAMM context. | Accept - Add clearer explanation in Methods: unadjusted = univariate models with single predictor; adjusted = full GAMM with all covariates. Explain how partial effects from full model represent 'adjusted' estimates. | Write new paragraph in Methods explaining unadjusted vs adjusted models, with reference to Figure 2 structure |
| 2 | Figure 3 is not properly described in the manuscript. Probably, 'Multi-country' is missing in the legend | Low - Figure 3 shows individual model effects. Need to check legend. | Accept - Review Figure 3 legend and caption. If 'Multi-country' label is missing, add it. Ensure caption clearly explains what is shown (partial effects by individual model). | Check Figure 3 generation code in descriptive.R and fix legend/caption |
| 2 | It is advisable to improve the model or consider additional features | High - This relates to QQ plot concerns (also raised by Reviewer 1). Suggests model may need additional features or different specification. | Partly accept - Acknowledge model limitations in Discussion. Note substantial unexplained variance (already mentioned). Could try alternative specifications (different link function, additional features) but major reanalysis may not be warranted if point estimates are robust. Emphasize exploratory nature of this work. | Add explicit discussion of model limitations and unexplained variance; review diagnostics and decide if sensitivity analyses are needed |
| 2 | Authors are advised to include ensemble model within the study as it often gives better results in epidemic forecasting | Medium-High - Ensemble models were explicitly excluded (stated in Methods). Including them would require reanalysis but could provide interesting comparison. | Reject - Hub ensemble models are constructed FROM the contributed models, creating circularity. They're not independent. Note in rebuttal that we excluded ensembles as stated in Methods, and they're methodologically distinct. Emphasize in Discussion that ensembles outperform (already noted line 291). | Check current text explaining ensemble exclusion; ensure rationale is clear. Prepare rebuttal explaining circular dependency |
| 3 | In the author summary perhaps a rewording about it being a large dataset. I agree it is large however, it was not large in the way needed to show the differences desired | Low-Medium - Author summary overstates sample size relative to statistical power. Good point about precision. | Accept - Revise author summary to acknowledge large number of forecasts but note limitation of relatively few independent models (especially per model type category). | Revise author summary sentence about dataset, adding caveat about sample size for detecting effects |
| 3 | It is not clear from the method of deciding the model classification whether both KS and SF processed all the files on the first pass | Low - Clarification of classification procedure. Current text says 'at least three investigators' and 'two investigators (SF, KS) separately reviewed all classifications'. | Accept - Clarify the procedure: all models classified by 3+ investigators independently, then SF and KS reviewed all classifications together to resolve. Make the sequence clearer. | Revise Methods section on classification to make procedure more explicit and sequential |
| 3 | It would be of interest to know where the disagreements existed was it spread across different classifications or was it mainly between a select two | Low-Medium - Additional methodological detail. Currently manuscript notes 17/47 models had disagreements but doesn't characterize pattern. | Accept - Add supplementary table or text describing disagreement patterns. If certain boundaries (e.g., mechanistic vs semi-mechanistic) were more ambiguous, this is useful context. | Analyze classification data to identify patterns in disagreements; add to supplement or main text |
| 3 | I wonder if in the supplement it could be included the results on the natural scale given it appears some of the work has been done | Low-Medium - Supplementary results. Manuscript mentions analyses on natural scale exist. Including them would support robustness. | Accept - Add supplementary figure/table showing results on natural scale, with note explaining differences from log-scale results (especially for semi-mechanistic models). | Check if natural-scale results exist in analysis files; generate figure/table for supplement |
| 3 | I wonder whether any models modelled multiple countries at once or whether all models that were multi country modelled them independently. This distinction seems of interest | Medium - Methodological detail about multi-country model structures. Important distinction between hierarchical multi-country models vs simply running same model per country. | Partly accept - Acknowledge we don't have detailed metadata on this. Add sentence noting this limitation and that it could explain some of the single vs multi-country differences. Suggest as future metadata to collect. | Add sentence in Discussion about not knowing whether multi-country models use hierarchical structures |
| 3 | is there something that can be learned or improved in the metadata files. I think it would be nice if the authors could describe the current issues and where effort can be made | Medium - Recommendations for future hub projects. Valuable contribution. | Accept - Add paragraph in Discussion with specific metadata recommendations based on this analysis: (1) clearer model structure descriptions, (2) documentation of model updates/changes, (3) target-specific customizations, (4) computational resources, (5) hierarchical structure for multi-location models. | Draft new Discussion paragraph with concrete metadata recommendations |
| Editor | We ask that a manuscript source file is provided at Revision. Please upload your manuscript file as a .doc, .docx, .rtf or .tex | Low - Technical submission requirement. | Accept - Ensure manuscript is uploaded in appropriate format (currently .docx which is acceptable). | Verify current file format meets requirements |
| Editor | Please upload all main figures as separate Figure files in .tif or .eps format | Low - Technical submission requirement. | Accept - Convert figures from current format to .tif or .eps and upload separately. | Export figures from R plotting code as .tif or .eps files |
| Editor | Please upload a copy of Figure figures S4 and S5 which you refer to in your text on pages 8, and 24. Or, if the figure is no longer to be included as part of the submission please remove all reference to it within the text | Low-Medium - Missing supplementary figures referenced in text. | Accept - Check if S4 and S5 exist (model diagnostics). If they exist, include them. If not, check what generates them (model-wis.R saves diagnostic plots) and ensure they're included, or remove references. | Locate or generate Supplementary Figures S4 and S5 (model diagnostics from GAMM) |

## Priority Summary

### High Priority (Major Impact on Content/Interpretation)
1. **Model specification concerns** (R1) - Location+time random effect and variant phases
2. **Hierarchical model structure** (R1) - Model/method relationship
3. **Model diagnostics/fit issues** (R1, R2) - QQ plot concerns
4. **Figure 1 misleading message** (R1) - Unadjusted results interpretation
5. **Target-specificity operationalization** (R1) - Proxy validity concern

### Medium Priority (Important for Clarity/Transparency)
1. **GAMM formula in main text** (R1, R2) - Transparency and reproducibility
2. **Terminology standardization** (R1) - Consistent use of technical terms
3. **GAMM justification** (R1, R2) - Why GAM vs simpler models
4. **Adjustment procedure explanation** (R2) - What "adjusted" means
5. **Metadata recommendations** (R3) - Future hub improvements

### Low Priority (Minor Fixes)
1. **Technical requirements** (Editor) - File formats, missing supplements
2. **Abstract wording** (R1) - Clarify terminology
3. **Table 1 improvements** (R1) - Remove unnecessary column
4. **R package versions** (R1) - Reproducibility details
5. **Figure design tweaks** (R1) - Color schemes

## Questions for Authors

Before finalizing the response, I'd like to clarify a few points:

1. **Variant phase analysis**: Do you have data on which variant was dominant when? Would it be feasible to conduct a sensitivity analysis with discrete time phases?

2. **Model diagnostics**: How concerning are the QQ plot deviations? Have you explored alternative link functions (identity vs log)?

3. **Ensemble exclusion**: The rationale seems clear (circularity), but Reviewer 2 may need explicit explanation. Can you confirm the ensemble was built from the same models being evaluated?

4. **Natural scale results**: Do these analyses already exist? Reviewer 3 suggests "some of the work has been done."

5. **Classification disagreements**: Do you have records of where disagreements occurred? This would address Reviewer 3's question.

6. **Population size analysis**: Would this be quick to run as a supplementary check for Reviewer 1?

7. **Supplementary Figures S4 and S5**: The editor notes these are referenced but missing. Were they generated by the model-wis.R code? Do they need to be compiled?
