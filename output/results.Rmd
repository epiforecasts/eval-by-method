---
title: "Results"
output: pdf_document
---

<!--- RMarkdown set up --->
```{r set-up, include=FALSE}
library(here)
library(dplyr)
library(readr)
library(tidyr)
library(tibble)
library(purrr)
library(ggplot2)
library(patchwork)
theme_set(theme_bw())
knitr::opts_chunk$set(eval = TRUE, echo = FALSE,
                      message = FALSE, warning = FALSE)
quantiles <- c(0.01, 0.25, 0.5, 0.75, 0.99)
```

```{r, eval=FALSE}
## NOT RUN
## Data creation pipeline
## Get metadata from googlesheet; save to data/
# source(here("R", "get-metadata.R"))
# get_metadata_processed()
## Get observed data and all Hub forecasts; exclude forecasts
# source(here("R", "import-data.R"))
## Score forecasts & ensembles on the log and natural scales; save to data/
# source(here("R", "score.R"))
```

```{r load-scores}
# Load score data
scores_raw <- read_csv(here("data", "scores-raw.csv"))
scores_pairwise <- read_csv(here("data", "scores-pw.csv"))
scores_pairwise_date <- read_csv(here("data",
                                      "scores-pw-target-date.csv")) 
scores_pairwise_horizon <- read_csv(here("data",
                                         "scores-pw-horizon.csv"))
```

<!--- Describe data and evaluation baseline --->

We evaluated 39 forecast models contributed to the European COVID-19 Forecast Hub, covering 32 countries over 104 weeks from 8 March 2021 to 10 March 2023. Here we report evaluations using log-transformed forecasts. We note this gives less extreme scores compared to using the natural scale (Figure 1; SI Figure 1).

We evaluated forecast performance against the performance of a median ensemble including all qualifying models. We observed the ensemble model generally outperformed individual models. On average across all forecast targets and horizons, only three models outranked the ensemble model (i.e., an average relative WIS <1). Average performance relative to the ensemble varied over time, with a median score of 1.39 and within an interquartile range of 1.07-1.85. Among 104 weeks of forecasts, the median score across all individual models outperformed the score of the ensemble model for only five weeks in autumn 2022.

```{r all-scores, fig.height = 6, fig.width = 10, fig.cap=fig1_cap}
scale_cols <- c("natural" = "#d95f02", "log" = "#1b9e77")
# All scores ----------------------
plot_all_scores <- scores_pairwise |> 
  filter(!grepl("EuroCOVIDhub-ensemble", model)) |> 
  arrange(rel_wis) |> 
  mutate(model = forcats::fct_inorder(model)) |> 
  pivot_wider(names_from = scale, values_from = rel_wis) |> 
  ggplot(aes(x = model)) +
  geom_point(aes(y = natural), 
             colour = scale_cols[["natural"]]) +
  geom_point(aes(y = log), 
             colour = scale_cols[["log"]]) +
  geom_linerange(aes(ymin = log, ymax = natural), 
                 colour = "grey70") +
  geom_hline(aes(yintercept = 1), lty = 2) +
  coord_flip() +
  labs(y = "Relative WIS", x = NULL) +
  theme(legend.position = "none")

# Plot over time ----------------------------------
scores_time <- scores_pairwise_date |> 
  filter(!grepl("EuroCOVIDhub-ensemble", model)) |> 
  group_by(target_end_date, scale) |>
  summarise(
    n = n(),
    value = quantile(rel_wis, quantiles),
    quantile = paste0("q", quantiles),
    model = "All models",
    .groups = "drop")

plot_all_scores_time <- scores_time |>
  pivot_wider(names_from = quantile) |>
  ggplot(aes(x = target_end_date, col = scale, fill = scale)) +
  geom_line(aes(y = q0.5), alpha = 0.5) +
  geom_ribbon(aes(ymin = q0.25, ymax = q0.75),
              alpha = 0.3, col = NA) +
    geom_ribbon(aes(ymin = q0.01, ymax = q0.99),
              alpha = 0.1, col = NA) +
  geom_hline(aes(yintercept = 1), lty = 2) +
  scale_y_log10() +
  scale_x_date(date_labels = "%b %Y") +
  scale_colour_manual(values = scale_cols, 
                      aesthetics = c("fill", "colour")) +
  labs(x = "Forecast target date", 
       y = "Relative WIS (log10)", 
       col = "Scale", fill = "Scale") +
  theme(legend.position = "bottom")

# Combine plots
plot_all <- plot_all_scores +
  plot_all_scores_time +
  patchwork::plot_annotation(tag_levels = "A") +
  patchwork::plot_layout(nrow = 1, widths = c(0.5, 1))

ggsave(plot = plot_all, filename = here("output", "figures", "figure-1.jpg"), height = 6, width = 10)

fig1_cap <- "Spread in the individual performance of 39 forecasting models compared to the ensemble model, using scaled relative weighted interval scores (WIS) scored on a log (green) or natural (orange) scale. Relative WIS is the geometric mean of the interval score, scaled relative to the ensemble (dashed line, value 1), with larger scores indicating worse performance relative to the ensemble. The log scale indicates forecasts and observations were transformed with the addition of a constant 1 before scoring. The natural scale indicates raw forecast and observed data were used. (A) Overall scores of each model across all forecast targets (predicting any of 32 locations, up to 4 weeks ahead, for 104 weeks). (B) Interquartile range across 39 modelsâ€™ scores by projection target date, showing median (line), with 50% and 99% quantile interval (shaded ribbons) across models."
plot_all
```

<!--- Describe models by model structure --->

```{r get-categories}
# Categorisation of models
# Get method types
metadata <- read_csv(here("data", "model-classification.csv")) |>
  select(model, method_type = classification)
# Identify number of targets for each model by week
targets_by_model <- read_csv(here("data", "targets-by-model.csv"))
```

```{r table-scores-by-method}
# Summarise overall performance by model structure
summary_quantiles <- c(0.5, 0.01, 0.99)
summary_by_method <- scores_pairwise |> 
  filter(!grepl("EuroCOVIDhub-ensemble", model)) |> 
  filter(scale == "log") |> 
  left_join(metadata) |> 
  filter(!grepl("Qualitative", method_type) & !is.na(method_type)) |> 
  group_by(method_type) |>
  reframe(
    n = n(),
    value = round(quantile(rel_wis, summary_quantiles), 2),
    quantile = paste0("q", summary_quantiles)) |> 
  pivot_wider(names_from = quantile) |> 
  mutate(score_range = paste0(q0.5, " (", q0.01, "-", q0.99, ")"))
```

Among 39 models, we defined three model structures. We categorised 8 models as statistical, 10 as semi-mechanistic, and 18 as mechanistic, with 3 qualitative ensemble models (table 1). No one model structure consistently outperformed the ensemble (figure 2A). Mechanistic models saw the widest range in performance, with the range of scores including both outperforming the ensemble and performing up to three times worse. Of the three models that, on average, outperformed the ensemble (figure 1), all were mechanistic and forecast for only one country (Poland or Italy). 

_Table 1. Description of model structures evaluated from the European COVID-19 Forecast Hub._

| Structure and examples 	| Description 	| Model count 	|
|---	|---	|---	|
| Statistical<br>ARIMA time series<br>LOESS seasonal-trend decomposition 	| Future data predicted entirely from past data. Data may be associated with the forecast target (such as incident cases or human behaviour), but a mechanism for this is not explicitly specified. 	| 8 	|
| Semi-mechanistic <br>SIR model with case convolution<br>State-space model based on the reproduction number<br> | Uses a combination of mathematical equations to describe epidemiological process, combined with data-driven associations | 10
| Mechanistic <br>SIR model<br>Age-stratified SEIR model with compartments for progressing disease severity<br>Agent based models: simulations of individuals within a system of defined relationships | Future data predicted entirely from the continuation of an underlying epidemiological process described in a set of mathematical equations. Data used to calibrate the initial model state to the scale of an epidemic, with fixed parameters for the process itself. | 18
| Qualitative | Qualitative ensembles: A combination of predictions made by models and qualitative human judgement (crowd forecasting). | 3


Over the length of the forecast horizon, the performance of statistical models appeared to worsen with increasing horizons up to four weeks. However, this was not the case for mechanistic and semi-mechanistic models. While the median score remained similar over time, the range of forecast scores around it narrowed, indicating more consistent performance with increasing forecast horizon. We noted some variation from these patterns when scores used forecasts on the natural, rather than the log, scale (SI Figure 1).

<!--- Describe models by target countries --->

```{r n-by-target}
# Summarise overall performance by target countries
# Count of model by countries
scores_targets <- scores_raw |> 
  filter(!grepl("EuroCOVIDhub-ensemble", model)) |>
  left_join(targets_by_model, by = c("model"))
target_single <- scores_targets |>
  filter(target_type == "Single-country") |>
  group_by(location) |>
  summarise(models = n_distinct(model)) |>
  arrange(desc(models))
target_multi <- scores_targets |>
  filter(target_type == "Multi-country") |>
  group_by(model) |>
  summarise(locations = n_distinct(location)) |>
  arrange(desc(locations))

# Summary performance by number of target country
summary_by_target <- scores_pairwise |> 
  filter(!grepl("EuroCOVIDhub-ensemble", model)) |> 
  filter(scale == "log") |> 
  left_join(targets_by_model) |>  
  group_by(target_type) |>
  reframe(
    n = n(),
    value = round(quantile(rel_wis, summary_quantiles), 2),
    quantile = paste0("q", summary_quantiles)) |> 
  pivot_wider(names_from = quantile) |> 
  mutate(score_range = paste0(q0.5, " (", q0.01, "-", q0.99, ")"))
```

We considered models forecasting for only one or multiple countries. We collated 23 multi-country models and 16 single-country models. Single-country models targeted Germany, Poland, Czech Republic, Spain, Italy, and Slovenia. Single-country models more often outperformed the ensemble than multi-country models, with a median (99% quantile interval) score of `r summary_by_target[summary_by_target$target_type=="Single-country","score_range"]` compared to `r summary_by_target[summary_by_target$target_type=="Multi-country","score_range"]`. Performance did not substantially vary with the forecast horizon, with both average and ranges remaining roughly similar for forecasts of one up to four weeks (figure 2B).

<!--- Plot performance by horizon --->

```{r plot-scores-by-method, fig.height=4, fig.width=4}
# --- Figure 2A ---
# Set up method types
method_factor <- c("Qualitative",
                   "Mechanistic", "Semi-mechanistic", "Statistical")
names(method_factor) <- method_factor

# Get scores by method type and horizon
scores_method <- scores_pairwise_horizon |> 
  filter(!grepl("EuroCOVIDhub-ensemble", model)) |>
  left_join(metadata, by = "model") |> 
  mutate(method_type = factor(method_type, 
                              method_factor, names(method_factor),
                              ordered = TRUE)) 

# Summarise by quantiles across Stat/Semi-/Mech models, point-score for others
scores_method_all <- scores_method |> 
  filter(!grepl("Qualitative", method_type)) |> 
  group_by(method_type, horizon, scale) |>
  summarise(
    n = n(),
    value = quantile(rel_wis, quantiles),
    quantile = paste0("q", quantiles),
    .groups = "drop")
scores_method_other <- scores_method |> 
  filter(grepl("Qualitative", method_type)) |> 
  select(model, method_type, horizon, value = rel_wis) |> 
  mutate(quantile = "q0.5", n = 1)

# Plot
plot_scores_method <- scores_method_all |> 
  bind_rows(scores_method_other) |> 
  filter(scale == "log") |> 
  pivot_wider(names_from = quantile) |>
  mutate(horizon = as.factor(horizon)) |> 
  ggplot(aes(y = method_type, col = horizon, fill = horizon)) +
  geom_point(aes(x = q0.5), alpha = 0.8, position = position_dodge(width = 1)) +
  geom_linerange(aes(xmin = q0.25, xmax = q0.75), linewidth = 4,
                 alpha = 0.5, position = position_dodge(width = 1)) +
    geom_linerange(aes(xmin = q0.01, xmax = q0.99), linewidth = 4,
                 alpha = 0.2, position = position_dodge(width = 1)) +
  geom_vline(xintercept = 1, lty = 2) +
    labs(y = NULL, x = NULL,
       col = "Week ahead forecast horizon",
       fill = "Week ahead forecast horizon") +
  scale_color_viridis_d(direction = 1) +
  theme(legend.position = "bottom")
```

```{r plot-scores-by-target, fig.height=4, fig.width=4}
# --- Figure 2B ---
# Get scores by target type and horizon
scores_targets <- scores_pairwise_horizon |> 
  filter(!grepl("EuroCOVIDhub-ensemble", model)) |>
  left_join(targets_by_model, by = "model")

# Summarise quantiles
scores_targets <- scores_targets |> 
  group_by(target_type, horizon, scale) |>
  summarise(
    n = n(),
    value = quantile(rel_wis, quantiles),
    quantile = paste0("q", quantiles),
    .groups = "drop")

# Plot
plot_scores_targets <- scores_targets |> 
  filter(scale == "log") |> 
  pivot_wider(names_from = quantile) |>
  mutate(horizon = as.factor(horizon)) |> 
  ggplot(aes(y = target_type, col = horizon, fill = horizon)) +
  geom_point(aes(x = q0.5), alpha = 0.8, position = position_dodge(width = 1)) +
  geom_linerange(aes(xmin = q0.25, xmax = q0.75), linewidth = 4,
                 alpha = 0.5, position = position_dodge(width = 1)) +
    geom_linerange(aes(xmin = q0.01, xmax = q0.99), linewidth = 4,
                 alpha = 0.2, position = position_dodge(width = 1)) +
  geom_vline(xintercept = 1, lty = 2) +
    labs(y = NULL, x = NULL,
       col = "Week ahead forecast horizon",
       fill = "Week ahead forecast horizon") +
  scale_color_viridis_d(direction = 1) +
  theme(legend.position = "none")
```

```{r plot-scores-by-both, fig.height=4, fig.width=8, fig.cap=fig2_cap}
# --- Figure 2 ---
# Join plots for method and target type
plot_method_target <- plot_scores_method +
  plot_scores_targets +
  patchwork::plot_annotation(tag_levels = "A") +
  patchwork::plot_layout(nrow = 1, guides = "collect") &
  theme(legend.position = "bottom")

ggsave(plot = plot_method_target, 
       filename = here("output",  "figures", "figure-2.jpg"),
       height = 4, width = 8)

fig2_cap <- "Performance of forecasting models by forecast horizon, by (A) model structure, (B) target specificity, showing the median relative weighted interval score (point), and the 50% and 99% quantile intervals (shaded ribbon), across all available models for each category. Forecasts were log-transformed and scored relative to an ensemble (dashed line, 1)."
plot_method_target
```

\newpage

<!--- Model rWIS --->

We used a generalised additive mixed effects model to identify the effects of model structure and target type on relative WIS. 

```{r model-wis}
source(here("R", "model-wis.R"))
# * Key to factors:
# Method: 1 = Mechanistic, 2 = Semi-mech, 3 = Statistical
# Target: 1 = Single country, 2 = Multi-country
# Trend: 1 = Stable, 2 = Increasing, 3 = Decreasing
# Model: 35 included (excl. 3 qualitative models)

# summary(m.fit) # Check output
# glance(m.fit) # Check fit
# m.smooths <- smooth_estimates(object = m.fit) # Get the smooth estimates
# draw(m.smooths) # Draw the smooth estimates
# appraise(m.fit) # QQ plot, residuals
# concurvity(m.fit, full = FALSE) # Concurvity
# variance_comp(m.fit) # Random effects as their equivalent variance components
```

!!! *Temporary results*

We found no substantial effect on forecast performance from either model structure or the number of countries forecast.

```{r print-gamm}
tidy(m.fit)
```

```{r plot-gamm}
draw(m.fit)
```
